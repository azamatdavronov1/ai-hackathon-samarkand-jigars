# The basic AI model

#This script builds a deep learning pipeline to classify chest X-ray images as Normal or Pneumonia using MobileNetV2 (a lightweight CNN architecture). It includes:
#âœ… Data preparation with automatic validation split
#ðŸ§ª Image augmentation for robust training
#ðŸ” Transfer learning using pretrained ImageNet weights
#ðŸ” Fine-tuning for improved performance
#ðŸ“Š Evaluation with test accuracy, classification report, and confusion matrix
#The model is trained on the Chest X-ray dataset and achieves strong accuracy using efficient and scalable methods.

import os
import shutil
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.model_selection import train_test_split

import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras.models import Model
from tensorflow.keras.layers import GlobalAveragePooling2D, Dense
from tensorflow.keras.optimizers import Adam

# === PATHS ===
BASE_DIR = "chest_xray"
TRAIN_DIR = os.path.join(BASE_DIR, "train")
CUSTOM_VAL_DIR = os.path.join(BASE_DIR, "custom_val")
TEST_DIR = os.path.join(BASE_DIR, "test")

IMG_SIZE = 224
BATCH_SIZE = 32
EPOCHS = 10

# === 1. Create a larger validation set by splitting train ===
def prepare_custom_val():
    if os.path.exists(CUSTOM_VAL_DIR):
        return  # Already created
    os.makedirs(CUSTOM_VAL_DIR, exist_ok=True)
    for cls in ['NORMAL', 'PNEUMONIA']:
        os.makedirs(os.path.join(CUSTOM_VAL_DIR, cls), exist_ok=True)
        files = os.listdir(os.path.join(TRAIN_DIR, cls))
        train_files, val_files = train_test_split(files, test_size=0.1, random_state=42)
        for file in val_files:
            src = os.path.join(TRAIN_DIR, cls, file)
            dst = os.path.join(CUSTOM_VAL_DIR, cls, file)
            shutil.copy(src, dst)

prepare_custom_val()

# === 2. Data Generators ===
train_datagen = ImageDataGenerator(rescale=1./255, 
                                   rotation_range=10,
                                   zoom_range=0.2,
                                   shear_range=0.1,
                                   horizontal_flip=True)

val_test_datagen = ImageDataGenerator(rescale=1./255)

train_gen = train_datagen.flow_from_directory(TRAIN_DIR,
                                              target_size=(IMG_SIZE, IMG_SIZE),
                                              batch_size=BATCH_SIZE,
                                              class_mode='binary')

val_gen = val_test_datagen.flow_from_directory(CUSTOM_VAL_DIR,
                                               target_size=(IMG_SIZE, IMG_SIZE),
                                               batch_size=BATCH_SIZE,
                                               class_mode='binary')

test_gen = val_test_datagen.flow_from_directory(TEST_DIR,
                                                target_size=(IMG_SIZE, IMG_SIZE),
                                                batch_size=1,
                                                class_mode='binary',
                                                shuffle=False)

# === 3. Build Model ===
base_model = MobileNetV2(input_shape=(IMG_SIZE, IMG_SIZE, 3),
                         include_top=False,
                         weights='imagenet')

base_model.trainable = False  # Freeze for initial training

x = GlobalAveragePooling2D()(base_model.output)
x = Dense(128, activation='relu')(x)
output = Dense(1, activation='sigmoid')(x)
model = Model(inputs=base_model.input, outputs=output)

model.compile(optimizer=Adam(learning_rate=0.0001),
              loss='binary_crossentropy',
              metrics=['accuracy'])

# === 4. Train (initial frozen layers) ===
model.fit(train_gen,
          validation_data=val_gen,
          epochs=5)

# === 5. Fine-tune (unfreeze some layers) ===
base_model.trainable = True
for layer in base_model.layers[:100]:  # Freeze first 100 layers
    layer.trainable = False

model.compile(optimizer=Adam(learning_rate=1e-5),  # Lower LR for fine-tuning
              loss='binary_crossentropy',
              metrics=['accuracy'])

model.fit(train_gen,
          validation_data=val_gen,
          epochs=5)
model.save('MobileNetV2.h5')

# === 6. Evaluate on test set ===
loss, acc = model.evaluate(test_gen)
print(f"\nâœ… Test Accuracy: {acc*100:.2f}%")

# === 7. Classification Report & Confusion Matrix ===
y_true = test_gen.classes
y_pred_probs = model.predict(test_gen)
y_pred = (y_pred_probs > 0.5).astype("int32").flatten()

print("\n=== Classification Report ===")
print(classification_report(y_true, y_pred, target_names=["Normal", "Pneumonia"]))

print("\n=== Confusion Matrix ===")
cm = confusion_matrix(y_true, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Predicted Normal', 'Predicted Pneumonia'],
            yticklabels=['Actual Normal', 'Actual Pneumonia'])
plt.xlabel("Prediction")
plt.ylabel("Truth")
plt.title("Confusion Matrix")
plt.show()
